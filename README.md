# ResearchAIde

AI-powered research assistant to help manage, search, and summarize research papers.

## Features

- **Upload and Process Papers:** Easily upload PDF research papers for processing and storage.
- **Semantic Search:** Find relevant papers or specific sections using advanced semantic search capabilities.
- **AI-Powered Summarization:** Get concise summaries of research content generated by AI.
- **Library Management:** Organize and manage your collection of research papers efficiently.
- **User-Friendly Interface:** Interact with your research data through an intuitive and easy-to-use interface.

## Project Structure

```
.
├── backend/          # FastAPI backend application (Python)
│   ├── app/          # Core application logic
│   │   ├── api/      # API endpoint definitions
│   │   ├── services/ # Business logic and integrations
│   │   ├── models/   # Pydantic models, data structures
│   │   └── main.py   # FastAPI app initialization
│   └── requirements.txt
├── frontend/         # React frontend application (TypeScript, Vite)
│   ├── src/          # Frontend source code
│   │   ├── components/ # Reusable UI components
│   │   ├── pages/    # Page-level components
│   │   ├── services/ # Frontend API clients, state management
│   │   └── main.tsx  # Frontend application entry point
│   └── package.json
├── data/             # Data storage (typically gitignored)
│   ├── chroma_db/    # Chroma vector database files
│   ├── papers/       # Original PDF paper storage
│   └── uploads/      # Temporary upload storage for new papers
├── docs/             # Project documentation
├── scripts/          # Utility and helper scripts
├── .env.example      # Example environment variables
├── .gitignore        # Specifies intentionally untracked files
├── .taskmasterconfig # Configuration for Taskmaster (if used)
└── README.md         # This file
```

## Technologies Used

- **Backend:** FastAPI, Python
- **Frontend:** React, Vite, TypeScript
- **Database:** ChromaDB (for vector storage)
- **Core AI/NLP:** Langchain, Sentence-Transformers

## Setup

### Prerequisites

- Python 3.9+
- Node.js 18+
- npm (or yarn, if preferred)

### Backend Setup

1.  Navigate to the `backend` directory:
    ```bash
    cd backend
    ```
2.  Create and activate a virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
3.  Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```

    Your backend API will typically be available at `http://localhost:8000` once started.

### Frontend Setup

1.  Navigate to the `frontend` directory:
    ```bash
    cd frontend
    ```
2.  Install dependencies:
    ```bash
    npm install
    # or
    # yarn install
    ```
    The frontend application, once started, will typically be available at `http://localhost:5173`.

### Environment Variables

1.  Copy the example environment file:
    ```bash
    cp .env.example .env
    ```
2.  Fill in the required values in the `.env` file.

## Running the Application

To run ResearchAIde, you'll need to start both the backend and frontend servers.

**Important:** Ensure the backend server is running before you start the frontend.

### Backend

1.  **Navigate to the backend directory:**
    ```bash
    cd backend
    ```
2.  **Activate your virtual environment:**
    ```bash
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
    *(If you haven't created one, refer to the "Backend Setup" section).*
3.  **Run the FastAPI application:**
    ```bash
    uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
    ```
    The backend API should now be running and accessible at `http://localhost:8000`.

### Frontend

1.  **Navigate to the frontend directory (in a new terminal):**
    ```bash
    cd frontend
    ```
2.  **Install dependencies (if you haven't already):**
    ```bash
    npm install
    # or
    # yarn install
    ```
3.  **Start the development server:**
    ```bash
    npm run dev
    # or
    # yarn dev
    ```
    The frontend application will typically be available at `http://localhost:5173` (Vite's default, but check your terminal output).

Once both are running, you can access the ResearchAIde application through your web browser by navigating to the frontend URL.

---

## Contributing

We welcome contributions to ResearchAIde! If you're interested in helping improve the project, please follow these general guidelines:

1.  **Fork the Repository:** Start by forking the main ResearchAIde repository to your own GitHub account.
2.  **Create a New Branch:** For each new feature or bug fix, create a new branch in your forked repository. This helps keep changes organized. A good branch name might be `feat/add-new-feature` or `fix/resolve-specific-bug`.
3.  **Make Your Changes:** Implement your changes, adhering to the project's coding style and conventions where possible.
4.  **Write Clear Commit Messages:** Commit your changes with clear, concise, and descriptive messages. This makes it easier to understand the history of changes.
5.  **Add Tests:** If you're adding a new feature or fixing a bug, please try to include relevant tests. This helps ensure stability and prevent regressions.
6.  **Open a Pull Request:** Once your changes are ready, open a pull request from your branch to the main ResearchAIde repository. Provide a clear description of the changes you've made and why.
7.  **Discuss (Optional but Recommended):** For significant changes, it's a good idea to check the existing issues or open a new issue to discuss your proposed changes before investing a lot of time. This can help align your contribution with the project's goals and avoid duplicate effort.

We appreciate your interest in contributing and look forward to your input!

---

## Vector Store Integration (ChromaDB)

The application uses ChromaDB as its local vector store for enabling semantic search and analysis of research papers. This integration allows for efficient storage, indexing, and retrieval of document chunks based on their semantic similarity to a query.

### Key Services

The core functionality is managed by the following services located in `backend/app/services/`:

*   **`VectorDBClient` (`vector_db_client.py`)**: Provides a low-level wrapper around the ChromaDB client. It handles the direct initialization of the `chromadb.PersistentClient` and offers basic methods for managing collections (create, list, get, delete) and data within them (add, query).

*   **`CollectionManager` (`collection_manager.py`)**: Sits on top of the `VectorDBClient` and enforces application-specific logic for research collections. This includes standardized naming conventions (e.g., `research_session_<session_id>`) and ensures required metadata (like `session_id`, `research_area`) is associated with collections.

*   **`EmbeddingService` (`embedding_service.py`)**: Responsible for generating vector embeddings from text. It currently uses local sentence-transformer models (e.g., `all-MiniLM-L6-v2`) but is designed to be extensible for other embedding providers.

*   **`ChunkingService` (`chunking_service.py`)**: Contains utilities for breaking down large text documents into smaller, manageable chunks suitable for embedding. Currently implements fixed-size chunking with overlap using `langchain_text_splitters`.

*   **`IngestionService` (`ingestion_service.py`)**: Orchestrates the entire document ingestion pipeline. It uses the `CollectionManager` to identify the target collection, the `ChunkingService` to process text, the `EmbeddingService` to generate embeddings, and the `VectorDBClient` to store the processed chunks, embeddings, and their metadata into ChromaDB.

*   **`SearchService` (`search_service.py`)**: Provides the semantic search functionality. It takes a user query and a session ID, generates a query embedding, and then queries the appropriate ChromaDB collection to find and return the most relevant document chunks, along with similarity scores.

### Configuration

The primary configuration for ChromaDB is its persistence path:

*   **`CHROMA_DB_PATH`**: Set this environment variable (e.g., in your `.env` file) to specify the directory where ChromaDB should store its data. If not set, it defaults to `data/chroma_db` relative to the project root (as defined in `backend/app/config.py`).

    Example `.env` entry:
    ```
    CHROMA_DB_PATH=./my_chroma_data/
    ```
    Ensure this path is writable and suitable for your environment.

### Basic Usage Examples

Below are conceptual examples of how these services might be used. (Note: These are illustrative and might require an async context or specific initialization steps not shown).

**1. Initializing Services (Conceptual)**

```python
# main.py or an initialization script
from backend.app.config import get_settings
from backend.app.services.vector_db_client import VectorDBClient
from backend.app.services.collection_manager import CollectionManager
from backend.app.services.embedding_service import EmbeddingService
from backend.app.services.ingestion_service import IngestionService
from backend.app.services.search_service import SearchService

settings = get_settings()

# Initialize core components
vector_db = VectorDBClient(settings=settings)
embedding_service = EmbeddingService() # Default model
collection_manager = CollectionManager(vector_db_client=vector_db)

# Initialize services that depend on the above
ingestion_service = IngestionService(
    vector_db_client=vector_db,
    collection_manager=collection_manager,
    embedding_service=embedding_service
)
search_service = SearchService(
    vector_db_client=vector_db,
    collection_manager=collection_manager,
    embedding_service=embedding_service
)

print("Services initialized.")
```

**2. Creating a Research Collection**

```python
# Assume collection_manager is initialized

session_id = "unique_session_abc123"
research_area = "Quantum Computing"
research_topic = "Entanglement Algorithms"

collection = collection_manager.create_research_collection(
    session_id=session_id,
    research_area=research_area,
    research_topic=research_topic
)

if collection:
    print(f"Collection '{collection.name}' created/accessed successfully.")
else:
    print(f"Failed to create/access collection for session '{session_id}'.")
```

**3. Ingesting a Document**

```python
# Assume ingestion_service and collection are set up

document_text = "This is the full text of a research paper about quantum entanglement..."
document_id = "paper_001"

# Ensure the collection for this session_id exists first (as shown in example 2)
success = ingestion_service.ingest_document(
    session_id=session_id, # Same session_id as collection creation
    document_id=document_id,
    document_text=document_text,
    document_metadata={"source": "arxiv_v1", "year": 2023}
)

if success:
    print(f"Document '{document_id}' ingested successfully into session '{session_id}'.")
else:
    print(f"Failed to ingest document '{document_id}'.")
```

**4. Performing a Semantic Search**

```python
# Assume search_service is initialized and documents have been ingested for the session_id

query = "What are the latest developments in quantum entanglement algorithms?"

# This is an async function
async def run_search():
    results = await search_service.semantic_search(
        session_id=session_id, # Same session_id used for ingestion
        query_text=query,
        n_results=5
    )

    if results:
        print(f"Found {len(results)} search results for query: '{query}'")
        for i, res in enumerate(results):
            print(f"  {i+1}. ID: {res['id']}, Score: {res['score']:.4f}")
            print(f"     Text: {res['text'][:100]}...")
            print(f"     Metadata: {res['metadata']}")
    else:
        print(f"No results found for query: '{query}'")

# Example of running the async search (in a real app, use an event loop manager)
# import asyncio
# asyncio.run(run_search())
```

This integration provides a robust foundation for semantic understanding and retrieval of textual data within the ResearchAIde application.
---
